:PROPERTIES:
:ID:       56c79c3b-49cb-4abc-99dc-22890d0e2cdc
:END:
#+title: Building a k8s cluster

- course is done on ubunut 18.04 LTS VMs

- [[https://jorgedelacruz.uk/2022/01/31/how-to-install-kubernetes-on-ubuntu-20-04/][Usefull link]]

* Installing Docker
*** Add Docker repo gpg key and the Docker repo
#+begin_src sh
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
    $(lsb_release -cs) \
    stable"
#+end_src

*** Reload the apt sources list, install and prevent auto-updates
#+begin_src sh
sudo apt update
sudo apt install -y docker-ce=18.06.1~ce~3-0~ubuntu
sudo apt-mark hold docker-ce
#+end_src

* Installing Kubeadm, Kubelet and Kubectl
*** Kubeadm
- tool which automates a large portion of the process of setting up a cluster

*** Kubelet
- esential component of Kubernetes that handles running containers on a node
- every server that will be running containers needs kubelet

*** Kubectl
- cli tool for interacting with the cluster once it is up
- used for managing the cluster

** Install
*** Add the k8s repo gpg key and the k8s repo
#+begin_src sh
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
#+end_src

*** Reload the apt sources list & install packages & prevent auto-updates
#+begin_src sh
sudo apt update
sudo apt install -y kubelet=1.22.6-00 kubeadm=1.22.6-00 kubectl=1.22.6-00
sudo apt-mark hold kubelet kubeadm kubectl
#+end_src


* Bootstrapping the Cluster

- create the file ~/etc/docker/daemon.json~:
    #+begin_src json
    {
    "exec-opts": ["native.cgroupdriver=systemd"]
    }
    #+end_src

    #+begin_src sh
    cat << EOF | sudo tee /etc/docker/daemon.json
    {
    "exec-opts": ["native.cgroupdriver=systemd"]
    }
    EOF
    #+end_src

*** init the cluster on the ~Kube Master~ server
#+begin_src sh
sudo kubeadm init --pod-network-cidr=10.244.0.0/16

# output
# I0414 10:43:59.923630   21924 version.go:255] remote version is much newer: v1.23.5; falling back to: st
# able-1.22
# [init] Using Kubernetes version: v1.22.8
# [preflight] Running pre-flight checks
# [preflight] Pulling images required for setting up a Kubernetes cluster
# [preflight] This might take a minute or two, depending on the speed of your internet connection
# [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
# [certs] Using certificateDir folder "/etc/kubernetes/pki"
# [certs] Using existing ca certificate authority
# [certs] Using existing apiserver certificate and key on disk
# [certs] Using existing apiserver-kubelet-client certificate and key on disk
# [certs] Using existing front-proxy-ca certificate authority
# [certs] Using existing front-proxy-client certificate and key on disk
# [certs] Using existing etcd/ca certificate authority
# [certs] Using existing etcd/server certificate and key on disk
# [certs] Using existing etcd/peer certificate and key on disk
# [certs] Using existing etcd/healthcheck-client certificate and key on disk
# [certs] Using existing apiserver-etcd-client certificate and key on disk
# [certs] Using the existing "sa" key
# [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
# [kubeconfig] Using existing kubeconfig file: "/etc/kubernetes/admin.conf"
# [kubeconfig] Using existing kubeconfig file: "/etc/kubernetes/kubelet.conf"
# [kubeconfig] Using existing kubeconfig file: "/etc/kubernetes/controller-manager.conf"
# [kubeconfig] Using existing kubeconfig file: "/etc/kubernetes/scheduler.conf"
# [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
# [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
# [kubelet-start] Starting the kubelet
# [control-plane] Using manifest folder "/etc/kubernetes/manifests"
# [control-plane] Creating static Pod manifest for "kube-apiserver"
# [control-plane] Creating static Pod manifest for "kube-controller-manager"
# [control-plane] Creating static Pod manifest for "kube-scheduler"
# [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
# [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
# [apiclient] All control plane components are healthy after 10.007101 seconds
# [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
# [kubelet] Creating a ConfigMap "kubelet-config-1.22" in namespace kube-system with the configuration for the kubelets in the cluster
# [upload-certs] Skipping phase. Please see --upload-certs
# [mark-control-plane] Marking the node b0415b8e872c.mylabserver.com as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
# [mark-control-plane] Marking the node b0415b8e872c.mylabserver.com as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
# [bootstrap-token] Using token: 3iuu3s.m6fd7i52dilzw1jk
# [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
# [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
# [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
# [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
# [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
# [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
# [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
# [addons] Applied essential addon: CoreDNS
# [addons] Applied essential addon: kube-proxy

# Your Kubernetes control-plane has initialized successfully!

# To start using your cluster, you need to run the following as a regular user:

#   mkdir -p $HOME/.kube
#   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
#   sudo chown $(id -u):$(id -g) $HOME/.kube/config

# Alternatively, if you are the root user, you can run:

#   export KUBECONFIG=/etc/kubernetes/admin.conf

# You should now deploy a pod network to the cluster.
# Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
#   https://kubernetes.io/docs/concepts/cluster-administration/addons/

# Then you can join any number of worker nodes by running the following on each as root:

# kubeadm join 172.31.119.182:6443 --token 3iuu3s.m6fd7i52dilzw1jk \
#         --discovery-token-ca-cert-hash sha256:0023677c46bec2280df7a5e5bd0eb1158adcc6bd6ff6cc0d425073737727401e
#+end_src

  - pod network cidr will be needed later for the flannel networking plugin

*** Set up kubeconfig for the local user
- do this on the Master Node
- allows usage of kubectl when logged in to the master

    #+begin_src sh
    mkdir -p $HOME/.kube
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config
    #+end_src

*** Run kubeadm join using command from kubeadm init
- run this command on slave nodes
#+begin_src sh
sudo kubeadm join $controller_ip:6443 --token $token --discovery-token-ca-cert-hash $hash

# output
cloud_user@b0415b8e871c:~$ sudo kubeadm join 172.31.119.182:6443 --token 3iuu3s.m6fd7i52dilzw1jk --discovery-token-ca-cert-hash sha256:0023677c46bec2280df7a5e5bd0eb1158adcc6bd6ff6cc0d425073737727401e
# [preflight] Running pre-flight checks
# [preflight] Reading configuration from the cluster...
# [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
# [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
# [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
# [kubelet-start] Starting the kubelet
# [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

# This node has joined the cluster:
# * Certificate signing request was sent to apiserver and a response was received.
# * The Kubelet was informed of the new secure connection details.

# Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
#+end_src

*** Confirm results
#+begin_src sh
cloud_user@b0415b8e872c:~$ kubectl get nodes
NAME                           STATUS     ROLES                  AGE     VERSION
b0415b8e871c.mylabserver.com   NotReady   <none>                 3m33s   v1.22.6
b0415b8e872c.mylabserver.com   NotReady   control-plane,master   13m     v1.22.6
b0415b8e873c.mylabserver.com   NotReady   <none>                 50s     v1.22.6
#+end_src


* Configuring Networking with Flannel
- on all three nodes run:

#+begin_src sh
echo "net.bridge.bridge-nf-call-iptables=1" | sudo tee -a /etc/sysctl.conf

sudo sysctl -p

# output
cloud_user@b0415b8e872c:~$ sudo sysctl -p
# net.bridge.bridge-nf-call-iptables = 1
#+end_src

- On ~Master Node~ use kubectl to install Flannel usin yaml template
#+begin_src sh
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

cloud_user@b0415b8e872c:~$ kubectl get nodes
# NAME                           STATUS   ROLES                  AGE   VERSION
# b0415b8e871c.mylabserver.com   Ready    <none>                 13m   v1.22.6
# b0415b8e872c.mylabserver.com   Ready    control-plane,master   23m   v1.22.6
# b0415b8e873c.mylabserver.com   Ready    <none>                 10m   v1.22.6

cloud_user@b0415b8e872c:~$ kubectl get pods -n kube-system
# NAME                                                   READY   STATUS    RESTARTS      AGE
# coredns-78fcd69978-hd5xn                               1/1     Running   0             24m
# coredns-78fcd69978-xsttx                               1/1     Running   0             24m
# etcd-b0415b8e872c.mylabserver.com                      1/1     Running   1 (25m ago)   24m
# kube-apiserver-b0415b8e872c.mylabserver.com            1/1     Running   1 (25m ago)   24m
# kube-controller-manager-b0415b8e872c.mylabserver.com   1/1     Running   1 (25m ago)   24m
# --------------------------------------------------------------------------------------------#
# kube-flannel-ds-427k5                                  1/1     Running   0             114s #
# kube-flannel-ds-5xk9k                                  1/1     Running   0             114s #
# kube-flannel-ds-fqzs6                                  1/1     Running   0             114s #
# --------------------------------------------------------------------------------------------#
# kube-proxy-7twr2                                       1/1     Running   0             12m
# kube-proxy-7zcm8                                       1/1     Running   0             24m
# kube-proxy-d2t5j                                       1/1     Running   0             14m
# kube-scheduler-b0415b8e872c.mylabserver.com            1/1     Running   1 (25m ago)   24m
#+end_src
